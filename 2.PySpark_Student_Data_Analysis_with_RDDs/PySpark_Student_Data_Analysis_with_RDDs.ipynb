{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f141aad-e500-4391-bc36-7b6ed43ea5f5",
   "metadata": {},
   "source": [
    "### Explore how the collect() operation works in PySpark using a dataset with basic RDD operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e024ebf4-994f-4117-85cd-83abd4d4ff3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.14:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ebfbc-48b4-4c0e-8991-514cb1e401c5",
   "metadata": {},
   "source": [
    "**Dataset Summary:**\n",
    "\n",
    "* **Number of entries:** 50\n",
    "* **Total features:** 7\n",
    "* The dataset contains student demographic information (**id, name, age, gender**) and academic performance data (**math, science, english**).\n",
    "\n",
    "| Feature Name | Description                                                                                                         | Data Type       | Example Value |\n",
    "| ------------ | ------------------------------------------------------------------------------------------------------------------- | --------------- | ------------- |\n",
    "| **id**       | Unique identifier for each student. Helps distinguish records.                                                      | Integer         | 1             |\n",
    "| **name**     | Name of the student. Serves as a label but is not useful for statistical analysis.                                  | String (Object) | Alice         |\n",
    "| **age**      | Age of the student in years. Useful for demographic insights and performance trends.                                | Integer         | 20            |\n",
    "| **gender**   | Gender of the student, typically denoted as ‘M’ (Male) or ‘F’ (Female). Allows gender-based performance comparison. | String (Object) | F             |\n",
    "| **math**     | Marks obtained by the student in Mathematics. Reflects proficiency in numerical and problem-solving skills.         | Integer         | 66            |\n",
    "| **science**  | Marks obtained in Science. Indicates understanding of scientific concepts and application.                          | Integer         | 92            |\n",
    "| **english**  | Marks obtained in English. Measures language comprehension, grammar, and writing ability.                           | Integer         | 44            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32259c95-fc92-452d-99a1-fcd187824589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "\n",
    "# # Initialize SparkContext\n",
    "# sc = SparkContext(\"local\", \"CSV_RDD_Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27b649b9-555a-4f67-9695-cae0b9b812bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV file (assuming students.csv is in working directory)\n",
    "data = sc.textFile(\"students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce3ec7a-f1f4-4eae-a90e-37ce73a32350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove header\n",
    "header = data.first()\n",
    "rows = data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1817351-4773-4fd9-b74e-6093eabe6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split by comma\n",
    "split_rdd = rows.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57e2aa5d-87fe-4de4-b64b-a65d21372f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Dataset (first 10 rows) ===\n",
      "['1', 'Alice', '20', 'F', '66', '92', '44']\n",
      "['2', 'Bob', '20', 'M', '82', '52', '77']\n",
      "['3', 'Charlie', '22', 'F', '43', '57', '76']\n",
      "['4', 'David', '19', 'M', '95', '69', '46']\n",
      "['5', 'Eva', '19', 'F', '62', '44', '96']\n",
      "['6', 'Frank', '22', 'F', '70', '78', '94']\n",
      "['7', 'Grace', '24', 'F', '67', '66', '93']\n",
      "['8', 'Henry', '21', 'F', '53', '82', '60']\n",
      "['9', 'Ivy', '19', 'M', '64', '52', '46']\n",
      "['10', 'Jack', '19', 'F', '44', '59', '60']\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Student Dataset (first 10 rows) ===\")\n",
    "for row in split_rdd.take(10):   # you can change 10 → 20, 50 etc.\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cf965d3-99f1-4594-8ec6-138a02eca007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Convert fields into structured format\n",
    "# (id, name, age, gender, math, science, english)\n",
    "students_rdd = split_rdd.map(lambda x: (int(x[0]), x[1], int(x[2]), x[3], int(x[4]), int(x[5]), int(x[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1703abd-f615-46b9-8763-652f9517ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calculate average marks for each student\n",
    "avg_marks_rdd = students_rdd.map(lambda x: (x[1], (x[4] + x[5] + x[6]) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0e9e5303-7350-4425-9144-b52fe90c0640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Filter students who scored avg >= 75\n",
    "passed_rdd = avg_marks_rdd.filter(lambda x: x[1] >= 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a75a0fc-1b0c-4313-a417-d33acf43286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Sort students by avg marks (descending)\n",
    "sorted_passed_rdd = passed_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08659284-13e6-43bb-a9ea-cb7198b67e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Collect results to driver\n",
    "results = sorted_passed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "329edd49-b6fa-40c0-ba4e-c32735f107e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Students with Average >= 75 ===\n",
      "Name: Leo, Avg Marks: 88.00\n",
      "Name: Olivia, Avg Marks: 88.00\n",
      "Name: Rita, Avg Marks: 86.67\n",
      "Name: Kathy, Avg Marks: 81.67\n",
      "Name: George, Avg Marks: 81.67\n",
      "Name: Frank, Avg Marks: 80.67\n",
      "Name: Oscar, Avg Marks: 80.00\n",
      "Name: Uma, Avg Marks: 78.33\n",
      "Name: Kyle, Avg Marks: 78.33\n",
      "Name: Matt, Avg Marks: 78.33\n",
      "Name: Tina, Avg Marks: 76.00\n",
      "Name: Victor, Avg Marks: 75.67\n",
      "Name: Grace, Avg Marks: 75.33\n",
      "Name: Mona, Avg Marks: 75.00\n",
      "Name: Will, Avg Marks: 75.00\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"=== Students with Average >= 75 ===\")\n",
    "for student in results:\n",
    "    print(f\"Name: {student[0]}, Avg Marks: {student[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60078a6a-6d9e-4dd3-8c09-31abecbc0a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of students who passed: 15\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Some extra RDD operations for practice\n",
    "# (a) Count how many students passed\n",
    "count_passed = passed_rdd.count()\n",
    "print(\"\\nNumber of students who passed:\", count_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f01bdd8a-d9d3-42af-b83c-338bd0340de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topper: ('Olivia', 88.0)\n"
     ]
    }
   ],
   "source": [
    "# (b) Find max average scorer\n",
    "topper = passed_rdd.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "print(\"Topper:\", topper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ffc85aa-06e3-4940-bca7-987a279cec2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 5 Passed Students (via take):\n",
      "[('Frank', 80.66666666666667), ('Grace', 75.33333333333333), ('Kathy', 81.66666666666667), ('Leo', 88.0), ('Mona', 75.0)]\n"
     ]
    }
   ],
   "source": [
    "# (c) Show first 5 passed students\n",
    "print(\"\\nFirst 5 Passed Students (via take):\")\n",
    "print(passed_rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad4982a1-37a4-4d0a-bc83-5c844685c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkContext\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fe16c2-c852-408b-853a-07c919abcc4c",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook provided a practical, step-by-step demonstration of a common data processing workflow using PySpark RDDs. We successfully loaded a raw CSV dataset, applied a series of **transformations** to clean, structure, and analyze the data, and finally used **actions** to trigger the computation and retrieve the results.\n",
    "\n",
    "The core of this exercise was to illustrate the role of the `collect()` action. It was used as the final step to gather the distributed results—the list of top-performing students—from all the **worker nodes** and bring them back to the **driver node** as a single Python list. This allowed us to easily iterate through and print the final, sorted results in a familiar way.\n",
    "\n",
    "Key takeaways from this exercise include:\n",
    "* **Lazy Evaluation**: Spark's transformations (like `map`, `filter`, and `sortBy`) are **lazy**. They don't execute until an action (like `collect`, `count`, or `take`) is called.\n",
    "* **The Power of `collect()`**: This action is essential for bringing a final, manageable result set back to the driver for local processing, reporting, or saving.\n",
    "* **Caution with `collect()`**: While useful, `collect()` must be used with care. It pulls the *entire* RDD into the driver's memory. Using it on a very large dataset can easily cause an `OutOfMemoryError`. For large-scale data, actions like `take()`, `saveAsTextFile()`, or `foreach()` are often safer alternatives.\n",
    "\n",
    "Overall, this notebook effectively showcases the fundamental pattern of building a data pipeline in Spark: chaining together transformations to define a processing logic and then calling an action to execute it and get the result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
