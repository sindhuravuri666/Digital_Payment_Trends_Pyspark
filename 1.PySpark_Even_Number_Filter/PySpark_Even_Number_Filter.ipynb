{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5720f49c-c1f0-420d-9c27-5d0fce4e8e0f",
   "metadata": {},
   "source": [
    "### Perform simple data transformation like filtering even numbers from a given list using PySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5f7165-8d75-474f-ab22-a5ccb2ddddb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://SubhapreetPatro.bbrouter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb76ac-431f-4dad-add9-c0c2c77b9f4e",
   "metadata": {},
   "source": [
    "**Dataset Summary:**\n",
    "\n",
    "* **Number of entries:** 50\n",
    "* **Total features:** 7\n",
    "* The dataset contains student demographic information (**id, name, age, gender**) and academic performance data (**math, science, english**).\n",
    "\n",
    "| Feature Name | Description                                                                                                         | Data Type       | Example Value |\n",
    "| ------------ | ------------------------------------------------------------------------------------------------------------------- | --------------- | ------------- |\n",
    "| **id**       | Unique identifier for each student. Helps distinguish records.                                                      | Integer         | 1             |\n",
    "| **name**     | Name of the student. Serves as a label but is not useful for statistical analysis.                                  | String (Object) | Alice         |\n",
    "| **age**      | Age of the student in years. Useful for demographic insights and performance trends.                                | Integer         | 20            |\n",
    "| **gender**   | Gender of the student, typically denoted as ‘M’ (Male) or ‘F’ (Female). Allows gender-based performance comparison. | String (Object) | F             |\n",
    "| **math**     | Marks obtained by the student in Mathematics. Reflects proficiency in numerical and problem-solving skills.         | Integer         | 66            |\n",
    "| **science**  | Marks obtained in Science. Indicates understanding of scientific concepts and application.                          | Integer         | 92            |\n",
    "| **english**  | Marks obtained in English. Measures language comprehension, grammar, and writing ability.                           | Integer         | 44            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8faedbfa-1aac-4a69-8d18-8b0e55db60ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "import random\n",
    "\n",
    "# Step 1: Initialize SparkContext\n",
    "# sc = SparkContext(\"local\", \"EvenNumberFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87344efa-e900-4b5e-97b3-3f3db810728b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List:\n",
      "[143, 686, 654, 592, 210, 355, 87, 440, 252, 747, 654, 397, 406, 884, 106, 360, 515, 169, 804, 163, 581, 518, 351, 534, 530, 683, 277, 33, 335, 592, 579, 9, 410, 81, 986, 76, 191, 172, 764, 974, 384, 884, 708, 120, 483, 654, 524, 506, 6, 367, 305, 973, 70, 780, 257, 761, 582, 529, 802, 19, 848, 690, 156, 653, 211, 171, 720, 322, 763, 249, 941, 665, 153, 683, 352, 309, 80, 256, 626, 851, 57, 689, 366, 877, 169, 504, 607, 938, 982, 242, 618, 315, 704, 550, 465, 103, 301, 323, 738, 610]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Generate 100 random integers between 1 and 1000\n",
    "random_numbers = [random.randint(1, 1000) for _ in range(100)]\n",
    "\n",
    "print(\"Original List:\")\n",
    "print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f88243-922e-46a4-9dcb-a049fd956894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Parallelize the list into an RDD\n",
    "numbers_rdd = sc.parallelize(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dd545f0-520c-4396-a624-b33ccb2e57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter only even numbers\n",
    "even_numbers_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341e2d62-9b16-4db1-92e3-10956599937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Even Numbers:\n",
      "[686, 654, 592, 210, 440, 252, 654, 406, 884, 106, 360, 804, 518, 534, 530, 592, 410, 986, 76, 172, 764, 974, 384, 884, 708, 120, 654, 524, 506, 6, 70, 780, 582, 802, 848, 690, 156, 720, 322, 352, 80, 256, 626, 366, 504, 938, 982, 242, 618, 704, 550, 738, 610]\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Collect results\n",
    "even_numbers = even_numbers_rdd.collect()\n",
    "\n",
    "print(\"\\nEven Numbers:\")\n",
    "print(even_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93f51e33-da19-4ddb-8987-a98caa473ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkContext\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429c2f7-d31b-4efc-a283-e5c81d33bc06",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This notebook successfully demonstrated a fundamental data processing task using PySpark. We began by generating a list of 100 random integers. This standard Python list was then transformed into a Resilient Distributed Dataset (RDD), the core data abstraction in Spark, using the `sc.parallelize()` method.\n",
    "\n",
    "The key operation was the application of the `filter()` transformation, which efficiently processed the distributed data in parallel to select only the numbers that satisfy the even number condition (`x % 2 == 0`). Finally, the `collect()` action was invoked to retrieve the filtered data from the distributed workers and bring it back to the driver program as a local list, which was then printed.\n",
    "\n",
    "This simple yet effective example illustrates the basic workflow of a Spark application:\n",
    "1.  Create an RDD from a data source.\n",
    "2.  Apply one or more transformations to the RDD.\n",
    "3.  Execute an action to trigger the computation and obtain a result.\n",
    "\n",
    "It serves as an excellent introduction to the power and simplicity of performing distributed data manipulation with PySpark."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
